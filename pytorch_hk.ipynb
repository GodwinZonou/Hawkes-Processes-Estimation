{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch import maximum\n",
    "from torch import minimum\n",
    "import Hawkes as hk\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4047\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Paramètres :\n",
    "mu, alpha, beta = 0.05, 1/4, 4\n",
    "T = 60000\n",
    "\n",
    "model = hk.simulator()\n",
    "model.set_kernel('exp')\n",
    "model.set_baseline('const')\n",
    "theta = {'mu':mu, 'alpha':alpha, 'beta':beta}\n",
    "model.set_parameter(theta)\n",
    "H_T = model.simulate([0,T])\n",
    "print(len(H_T))\n",
    "print(type(H_T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExcitationKernel(nn.Module):\n",
    "    def _init_(self, p=100):\n",
    "        super()._init_()\n",
    "        self.A1 = nn.Linear(1, p)\n",
    "        self.A2 = nn.Linear(p, 1)\n",
    "\n",
    "        # Initialisation des poids des couches linéaires\n",
    "        init.uniform_(self.A1.weight, a=0, b=0.5)\n",
    "        init.uniform_(self.A2.weight, a=-0.5, b=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x)\n",
    "        x = F.relu(self.A1(x))\n",
    "        print(x)\n",
    "        x = self.A2(x)\n",
    "        print(x)\n",
    "        return torch.exp(x)\n",
    "\n",
    "def phi(t, b1, W2, W1, b2):\n",
    "    #print('ici0', np.exp(b2 + np.dot(W2, np.maximum(W1*t + b1, 0))))\n",
    "    return torch.exp(b2 + np.dot(W2, maximum(W1*t + b1, 0)))\n",
    "\n",
    "def calculer_lambda(t, mu, b1, W2, W1, b2, H_T):\n",
    "    lambda_val = mu\n",
    "    for tau in H_T:\n",
    "        if tau < t:\n",
    "            lambda_val += phi(t - tau, b1, W2, W1, b2)\n",
    "        else:\n",
    "            break\n",
    "    return lambda_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LogLikelihood():\n",
    "    def __init__(self, W1, W2, b1, b2, mu, phi,T):\n",
    "        super(LogLikelihood, self).__init__()\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "        self.mu = mu\n",
    "        self.phi = phi\n",
    "        self.T=T\n",
    "\n",
    "    def calculer_lambda(self, t, H_T):\n",
    "        lambda_val = self.mu\n",
    "        phi=self.phi\n",
    "        for tau in H_T:\n",
    "            if tau < t:\n",
    "                lambda_val += phi(t - tau)\n",
    "            else:\n",
    "                break\n",
    "        return torch.tensor(lambda_val)\n",
    "\n",
    "    def calculate_integral(self, t):\n",
    "\n",
    "        W1 = self.W1\n",
    "        W2 = self.W2\n",
    "        b1 = self.b1\n",
    "        b2 = self.b2\n",
    "        mu = self.mu\n",
    "        phi = self.phi\n",
    "\n",
    "        #W1 = np.array(W1)  \n",
    "        #b1 = np.array(b1)  \n",
    "        #W2 = np.array(W2)\n",
    "    \n",
    "        s_inflection_points = -b1 / W1.t()\n",
    "        s_inflection_points = torch.sort(s_inflection_points).values\n",
    "    \n",
    "        integral_value = 0.0\n",
    "        eps = 10**(-6)\n",
    "\n",
    "# Find the largest subsequence of sorted inflection points within [0, t]\n",
    "\n",
    "        s_l = torch.max(s_inflection_points[s_inflection_points <= t])\n",
    "        s_u = torch.min(s_inflection_points[s_inflection_points >= 0.0])\n",
    "\n",
    "        # Calculate the integral by summing over segments between consecutive inflection points\n",
    "        for i in range(len(s_inflection_points) - 1):\n",
    "            s_m = s_inflection_points[i]\n",
    "            s_n = s_inflection_points[i + 1]\n",
    "            if s_m > 0 and s_n < t:\n",
    "                #print([W2[j] * W1[j] for j in range(len(W2)) if W1[j] * (s_n - eps) + b1[j] > 0])\n",
    "                integral_value += (phi(s_n) - phi(s_m)) / (\n",
    "                    torch.sum(torch.tensor(W2 * W1 *(W1 * (s_n - eps) + b1 > 0).float()))\n",
    "                )\n",
    "\n",
    "        integral_value += (phi(s_u) - phi(0)) / (\n",
    "                    torch.sum(W2 * W1 *(W1 * (s_u - eps) + b1 > 0).float())\n",
    "                )\n",
    "\n",
    "        integral_value += (phi(t) - phi(s_l)) / (\n",
    "                    torch.sum(W2 * W1 *(W1 * (t - eps) + b1 > 0).float())\n",
    "                )\n",
    "        #W1 = torch.tensor(W1, dtype=torch.float32)  \n",
    "        #b1 = torch.tensor(b1, dtype=torch.float32)  \n",
    "        #W2 = torch.tensor(W2, dtype=torch.float32)\n",
    "        \n",
    "\n",
    "        return integral_value\n",
    "    \n",
    "    def loss_final(self, t):\n",
    "        loss_value = torch.tensor(0.0)\n",
    "        mu = self.mu\n",
    "        H_T=self.T\n",
    "        if H_T.any():\n",
    "            H = torch.tensor(np.array([t_n for t_n in H_T if t_n < t]))\n",
    "            for i in range(1, H.size(0)):\n",
    "                loss_value += torch.log(self.calculer_lambda(H[i], H)) - mu * (H[i] - H[i - 1]) - self.calculate_integral(t - H[i])\n",
    "            loss_value += torch.log(self.calculer_lambda(H[0], H)) - mu * (H[0]) - self.calculate_integral(t - H[0])\n",
    "        return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 4929.4902\n",
      "Epoch [2/30], Loss: 4929.4902\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m phi_chap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m a: model(torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray([a]), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Passage avant : calcul de la perte\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mLogLikelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfc1.weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfc2.weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfc1.bias\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfc1.bias\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43mphi_chap\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_final\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Rétropropagation : calcul des gradients\u001b[39;00m\n\u001b[0;32m     58\u001b[0m loss\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[24], line 77\u001b[0m, in \u001b[0;36mLogLikelihood.loss_final\u001b[1;34m(self, t)\u001b[0m\n\u001b[0;32m     75\u001b[0m     H \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray([t_n \u001b[38;5;28;01mfor\u001b[39;00m t_n \u001b[38;5;129;01min\u001b[39;00m H_T \u001b[38;5;28;01mif\u001b[39;00m t_n \u001b[38;5;241m<\u001b[39m t]))\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, H\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)):\n\u001b[1;32m---> 77\u001b[0m         loss_value \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculer_lambda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m-\u001b[39m mu \u001b[38;5;241m*\u001b[39m (H[i] \u001b[38;5;241m-\u001b[39m H[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_integral(t \u001b[38;5;241m-\u001b[39m H[i])\n\u001b[0;32m     78\u001b[0m     loss_value \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculer_lambda(H[\u001b[38;5;241m0\u001b[39m], H)) \u001b[38;5;241m-\u001b[39m mu \u001b[38;5;241m*\u001b[39m (H[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_integral(t \u001b[38;5;241m-\u001b[39m H[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_value\n",
      "Cell \u001b[1;32mIn[24], line 17\u001b[0m, in \u001b[0;36mLogLikelihood.calculer_lambda\u001b[1;34m(self, t, H_T)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tau \u001b[38;5;129;01min\u001b[39;00m H_T:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tau \u001b[38;5;241m<\u001b[39m t:\n\u001b[1;32m---> 17\u001b[0m         lambda_val \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mphi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[31], line 52\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# # Boucle d'entraînement\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;66;03m#optimizer_fc1.zero_grad()  # Remise à zéro des gradients\u001b[39;00m\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;66;03m#optimizer_fc2.zero_grad()\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m         phi_chap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m a: \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;66;03m# Passage avant : calcul de la perte\u001b[39;00m\n\u001b[0;32m     55\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mLogLikelihood(model\u001b[38;5;241m.\u001b[39mstate_dict()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc1.weight\u001b[39m\u001b[38;5;124m'\u001b[39m],model\u001b[38;5;241m.\u001b[39mstate_dict()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc2.weight\u001b[39m\u001b[38;5;124m'\u001b[39m],model\u001b[38;5;241m.\u001b[39mstate_dict()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc1.bias\u001b[39m\u001b[38;5;124m'\u001b[39m],model\u001b[38;5;241m.\u001b[39mstate_dict()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc1.bias\u001b[39m\u001b[38;5;124m'\u001b[39m],mu,phi_chap,x)\u001b[38;5;241m.\u001b[39mloss_final(\u001b[38;5;241m60000\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\zozow\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zozow\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[31], line 19\u001b[0m, in \u001b[0;36mSimpleNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 19\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m     21\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(out)\n",
      "File \u001b[1;32mc:\\Users\\zozow\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zozow\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zozow\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Définir le réseau de neurones\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Initialisation des poids de la couche d'entrée de manière aléatoire\n",
    "        init.uniform(self.fc1.weight, 0.0,0.5)\n",
    "        init.constant_(self.fc1.bias, 0.1)  # Initialisation des biais à zéro\n",
    "\n",
    "        # Initialisation des poids de la couche de sortie de manière aléatoire\n",
    "        init.uniform(self.fc2.weight, -0.5,0)\n",
    "        init.constant_(self.fc2.bias, 0.1)  # Initialisation des biais à zéro\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = torch.exp(out)\n",
    "        return out\n",
    "\n",
    "# Définir les paramètres du réseau de neurones\n",
    "input_size = 1\n",
    "hidden_size = 100\n",
    "output_size = 1\n",
    "\n",
    "# Créer une instance du réseau de neurones\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "model.fc1.weight.requires_grad_(True)\n",
    "model.fc1.bias.requires_grad_(True)\n",
    "\n",
    "\n",
    "# Spécification des taux d'apprentissage pour chaque couche\n",
    "learning_rate_fc1 = 1e-5  # Taux d'apprentissage pour la couche d'entrée\n",
    "learning_rate_fc2 = 1e-2  # Taux d'apprentissage pour la couche de sortie\n",
    "\n",
    "# Définition des optimiseurs pour chaque couche avec des taux d'apprentissage différents\n",
    "optimizer_fc1 = optim.Adam(model.fc1.parameters(), lr=learning_rate_fc1)\n",
    "optimizer_fc2 = optim.Adam(model.fc2.parameters(), lr=learning_rate_fc2)\n",
    "mu=0.05\n",
    "\n",
    "\n",
    "\n",
    "x=torch.tensor(H_T,dtype=torch.float32)\n",
    "# # Boucle d'entraînement\n",
    "for epoch in range(30):\n",
    "        #optimizer_fc1.zero_grad()  # Remise à zéro des gradients\n",
    "        #optimizer_fc2.zero_grad()\n",
    "        phi_chap = lambda a: model(torch.tensor(np.array([a]), dtype=torch.float32)).item()\n",
    "\n",
    "        # Passage avant : calcul de la perte\n",
    "        loss = -LogLikelihood(model.state_dict()['fc1.weight'],model.state_dict()['fc2.weight'],model.state_dict()['fc1.bias'],model.state_dict()['fc1.bias'],mu,phi_chap,x).loss_final(60000)\n",
    "    \n",
    "        # Rétropropagation : calcul des gradients\n",
    "        loss.requires_grad_(True)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Mise à jour des poids\n",
    "        optimizer_fc1.step()\n",
    "        optimizer_fc2.step()\n",
    "        \n",
    "        # Afficher la perte à chaque époque\n",
    "        print(f'Epoch [{epoch+1}/{30}], Loss: {loss.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
